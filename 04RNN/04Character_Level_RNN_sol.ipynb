{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Level LSTM in PyTorch\n",
    "\n",
    "이번 실습을 통해 character-level LSTM 를 구현하자. 특정 text에서 문자를 하나씩 학습한다. 그리고 학습한 것을 바탕으로 새로운 문자를 생성하여 문장을 만든다. Anna Karenina라는 소설을 사용하여 모형을 구현해 보자. **소설을 학습하고 소설과 유사한 문장을 생성하는 실습을 해보자.**\n",
    "\n",
    "아래 그림은 일반적인 character-wise RNN의 구조를 나타낸다.\n",
    "\n",
    "![img](../assets/charseq.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data\n",
    "\n",
    "Anna Karenina text file을 다운로드하여 학습에 사용할 수 있도록 전처리를 수행한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('./data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음 100 개 문자를 확인해 보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "문자를 숫자로 전환하기 위한 **dictionary**를 생성한다. 문자를 숫자로 Encoding하여 모형의 input data로 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음 100개 문자를 확인하여 문자가 숫자로 인코딩 되었음을 확인해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([54, 28, 58, 59, 46, 56, 22, 47, 70, 45, 45, 45, 75, 58, 59, 59, 77,\n",
       "       47, 38, 58, 79, 57,  7, 57, 56,  0, 47, 58, 22, 56, 47, 58,  7,  7,\n",
       "       47, 58,  7, 57, 44, 56, 35, 47, 56, 63, 56, 22, 77, 47, 52, 40, 28,\n",
       "       58, 59, 59, 77, 47, 38, 58, 79, 57,  7, 77, 47, 57,  0, 47, 52, 40,\n",
       "       28, 58, 59, 59, 77, 47, 57, 40, 47, 57, 46,  0, 47, 74,  4, 40, 45,\n",
       "        4, 58, 77, 20, 45, 45,  1, 63, 56, 22, 77, 46, 28, 57, 40],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data\n",
    "\n",
    "LSTM에서는 input을 **one-hot encoded** 으로 사용한다. 따라서 우리가 인코딩한 문자도 one-hot encoding 방식으로 변환해야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches\n",
    "\n",
    "\n",
    "모형을 훈련하기 위해 mini-batches 를 생성한다. 생성되는 batches 는 아래와 비슷하게 될 것이다:\n",
    "\n",
    "![img](../assets/sequence_batching@1x.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "이 예에서 encoded characters 를 `arr` 라고 하자. 이를 주어진 `batch_size`로 multiple sequences로 나눈다, 각각의 sequences는 `seq_length` 길이를 가진다.\n",
    "\n",
    "### Creating Batches\n",
    "\n",
    "**1. 먼저 full mini-batches로 나누고 남은 문자는 모두 삭제한다.**\n",
    "\n",
    "각 batch는 $N \\times M$ 의 문자를 가진다.(여기서 $N$ 은 batch size = 하나의 batch에 들어가는 sequence 개수) 그리고 $M$ 은 sequence_length 이다. 그 다음에 batches의 총 갯수인 $K$는 `arr` 의 길이를 하나의 batch에 들어가는 문자의 갯수(=$N \\times M$)로 나누면 된다. `arr`에서 필요한 총 문자의 갯수는 $N * M * K$이 된다.\n",
    "\n",
    "**2. `arr`를 $N$ batches로 나눈다.** \n",
    "\n",
    "`arr.reshape(size)`를 사용한다. `size` tuple값으로 준다. 하나의 batch 당 $N$ sequences가 있다. 따라서 $N$이 첫번째 차원값이 된다. reshape를 하고 나면 $N \\times (M * K)$이 된다.\n",
    "\n",
    "**3. 이 array를 사용하여 mini-batches를 생성한다.**\n",
    "\n",
    "$N \\times (M * K)$ array에 대해서 각각의 batch $N \\times M$ window를 가진다. 윈도우는`seq_length`만큼 이동한다. input array와 target arrays를 생성한다. targets은 단지 inputs 을 one character shift한 것이다. `range` 함수를 사용하는데 이때 interval option을 seq_length로 주면 된다. \n",
    "\n",
    "> **실습 :** 아래에 batches 를 생성하는 function를 작성하자. 쉽지 않은 실습이므로 해답 내용을 참조하여 작성해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    ## TODO: Get the number of batches we can make\n",
    "    n_batches = len(arr) // (batch_size*seq_length) #정수나누기해야한다.\n",
    "    \n",
    "    ## TODO: Keep only enough characters to make full batches\n",
    "    arr = arr[:batch_size*seq_length*n_batches]\n",
    "    \n",
    "    ## TODO: Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    ## TODO: Iterate over the batches using a window of size seq_length\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one ==> y는 target으로 x값을 1만큼 shift해서 생성한다\n",
    "        y = np.zeros_like(x)\n",
    "        try :\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]            \n",
    "        yield x, y\n",
    "        # yield는 generator를 return한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Your Implementation\n",
    "\n",
    "batch가 제대로 생성되는지 테스트 해본다. 일단 batch size 는 8 그리고 50 sequence length로 설정해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[54 28 58 59 46 56 22 47 70 45]\n",
      " [ 0 74 40 47 46 28 58 46 47 58]\n",
      " [56 40 41 47 74 22 47 58 47 38]\n",
      " [ 0 47 46 28 56 47 24 28 57 56]\n",
      " [47  0 58  4 47 28 56 22 47 46]\n",
      " [24 52  0  0 57 74 40 47 58 40]\n",
      " [47 48 40 40 58 47 28 58 41 47]\n",
      " [71 33  7 74 40  0 44 77 20 47]]\n",
      "\n",
      "y\n",
      " [[28 58 59 46 56 22 47 70 45 45]\n",
      " [74 40 47 46 28 58 46 47 58 46]\n",
      " [40 41 47 74 22 47 58 47 38 74]\n",
      " [47 46 28 56 47 24 28 57 56 38]\n",
      " [ 0 58  4 47 28 56 22 47 46 56]\n",
      " [52  0  0 57 74 40 47 58 40 41]\n",
      " [48 40 40 58 47 28 58 41 47  0]\n",
      " [33  7 74 40  0 44 77 20 47 67]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Defining the network with PyTorch\n",
    "\n",
    "아래와 같은 network를 구성해 보자.\n",
    "\n",
    "![img](../assets/charRNN.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Structure\n",
    "\n",
    "`__init__` 는 다음과 같이 작성한다:\n",
    "* 필요한 dictionary를 생성한다.\n",
    "* LSTM layer는 input size (characters 갯수), hidden layer size `n_hidden`, layers 갯수 `n_layers`, dropout 확률로 `drop_prob`, 그리고 batch_first = True로 설정한다.\n",
    "* dropout layer 를 설정한다.\n",
    "* fully-connected layer는 input size `n_hidden`와 output size (characters의 갯수)로 생성한다.\n",
    "* 최종적으로 weight를 초기화한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### LSTM Inputs/Outputs\n",
    "\n",
    "기본적으로 [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) 은 다음과 같이 작성한다.\n",
    "\n",
    "```python\n",
    "self.lstm = nn.LSTM(input_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "```\n",
    "\n",
    "`input_size` 는 characters의 갯수이다. sequential input을 받고 `n_hidden`는 hidden layers 에서의 unit 개수이다. dropout을 설정할 수 있다. 마지막으로 `forward` function에서 LSTM cells을 쌓아 올린다.\n",
    "\n",
    "hidden state 의 초기 상태는 모두 0으로 초기화한다.\n",
    "\n",
    "```python\n",
    "self.init_hidden()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the layers of the model\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden) # lstm 수행하면 수행 결과와 새로운 히든이 출력으로.\n",
    "        \n",
    "        out = self.dropout(r_output) # lstm출력을 dropout하여 \n",
    "        \n",
    "        out = out.contiguous().view(-1, self.n_hidden) # shape 바꾸어서 다음 layer로 stack\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        # return the final output and the hidden state\n",
    "       \n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "  \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train\n",
    "\n",
    "훈련하기 위한 epochs수와 learning rate,그리고 기타 parameters를 적절히 설정한다.\n",
    "\n",
    "Adam optimizer 와 cross entropy loss 를 사용한다. \n",
    " \n",
    "> * gradient가 지수 함수로 증가하는 경우가 있을 수 있다. 이 문제는 exploding gradient 문제로 잘 알려져 있다. [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) 를 사용하여  gradients exploding을 방지한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()    \n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    \n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the model\n",
    "\n",
    "Network instance를 생성하고 hyperparameters를 설정한다. 그리고 mini-batches sizes를 설정하고 training한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## TODO: set you model hyperparameters\n",
    "# define and print the net\n",
    "n_hidden= 512\n",
    "n_layers= 2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your training hyperparameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 10... Loss: 3.2768... Val Loss: 3.2368\n",
      "Epoch: 1/1... Step: 20... Loss: 3.1438... Val Loss: 3.1355\n",
      "Epoch: 1/1... Step: 30... Loss: 3.1416... Val Loss: 3.1234\n",
      "Epoch: 1/1... Step: 40... Loss: 3.1145... Val Loss: 3.1191\n",
      "Epoch: 1/1... Step: 50... Loss: 3.1443... Val Loss: 3.1169\n",
      "Epoch: 1/1... Step: 60... Loss: 3.1202... Val Loss: 3.1156\n",
      "Epoch: 1/1... Step: 70... Loss: 3.1051... Val Loss: 3.1135\n",
      "Epoch: 1/1... Step: 80... Loss: 3.1223... Val Loss: 3.1092\n",
      "Epoch: 1/1... Step: 90... Loss: 3.1138... Val Loss: 3.0970\n",
      "Epoch: 1/1... Step: 100... Loss: 3.0803... Val Loss: 3.0663\n",
      "Epoch: 1/1... Step: 110... Loss: 3.0323... Val Loss: 3.0133\n",
      "Epoch: 1/1... Step: 120... Loss: 2.9059... Val Loss: 2.8930\n",
      "Epoch: 1/1... Step: 130... Loss: 2.8215... Val Loss: 2.7923\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 1 # start small if you are just testing initial behavior\n",
    "\n",
    "print('Training the model...')\n",
    "# train the model\n",
    "train(net, encoded, \n",
    "      epochs=n_epochs, batch_size=batch_size, \n",
    "      seq_length=seq_length, lr=0.001,\n",
    "      print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "설정이 필요한 hyperparameters 는 다음과 같다..\n",
    "\n",
    "* `n_hidden` - The number of units in the hidden layers.\n",
    "* `n_layers` - Number of hidden LSTM layers to use.\n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `seq_length` - Number of characters in the sequence. 보통 큰 값으로 설정하면 더 긴 내용을 학습할 수 있다.너무 크게 하면 학습이 오래 걸린다. \n",
    "* `lr` - Learning rate for training\n",
    "\n",
    "## Tips and Tricks\n",
    "\n",
    ">### Validation Loss vs. Training Loss을 확인한다.\n",
    ">- 보통 training loss와 validation loss 의 차이가 심하면 **overfitting**된다고 본다. 그런 경우에는 network size를 줄이거나 dropout을 설정한다.\n",
    ">- 만약, training/validation loss가 같으면 **underfitting**되었다고 본다. 그럴 경우에는 layer의 갯수, layer당 unit의 갯수를 증가한다.\n",
    "\n",
    ">### 적절한 hyper parameter를 설정한다.\n",
    "> 중요한 parameters는 `n_hidden`,`n_layers` 2개 이다. 이 parameter는 데이터셋의 크기에 따라 달라진다. 보통의 경우에는 전체 train resource를 고려하여 좀 더 큰 네트워크를 만들어서 train을 진행한다. validation, train loss를 살펴보고 overfitting 되면 dropout 등을 추가적으로 주고 훈련을 진행한다. 이때 validation loss가 최소가 되는 모형을 최종적인 모형으로 저장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "훈련이 종료되면 최종적인 모형을 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_1_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Making Predictions\n",
    "\n",
    "모형이 잘 작동하여 문장을 제대로 생성하는지 확인해 본다.\n",
    "\n",
    "### A note on the `predict`  function\n",
    "\n",
    "RNN의 출력 값은 주어진 입력값 다음에 나올 문자를 예측하여 출력한다. 문자의 score값을 출력하므로 이를 확률로 전환해야 한다. 확률로 전환하기 위해 softmax함수를 사용한다.\n",
    "\n",
    "> softmax function를 적용하면 주어진 입력 다음에 나올 문자에 대한 확률값을 출력하게 된다.\n",
    "\n",
    "### Top K sampling\n",
    "\n",
    "다음 나올 문자의 확률이 높은 k개의 문자만이 최종 출력되도록 해야한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    " \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priming and generating text \n",
    "\n",
    "보통 문장의 시작 단어를 prime 값(=초기값)으로 준다. 그러면 이 문자열을 기반으로 하여 문장을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annas\n",
      " ootied he oe an oose he aas hed\n",
      "taree he tatin an to sine hadd aosse nas tetin tise ha hera had an oo  hater natian the hh  hetthes ond aetins hosinn ate ande atit oad, oan he tian tan ansee ahed aad, ans ans het hind tisdinsetisesiod toe sasind, her nnt an shi tone hod hertit attat thad sines hes ind ate ne hhr se hasdised hir hottes hhe an tote aa tan thes ote ter to sisro tare he ae sintes ho teet at aons herteed terereta toteestin aat ho ao ae ho an sin toee hor tetittere tond, thot ion the sori ae hede hantia shis hin ao has tine te aane hes oteire sat ios tins he she tontee hit tae heteran tae atrettat intiad tore toesitistit ind oad ate sititrin  hored hotie ten oo the toed so se sha he toesse tho an ho hod the seese thes ot at ho had waterar tat tot an san she sar san aad tene thr to hh aes ae ood hod an tane ton he aodd an hira hhe his oas aer tat he hh  he het oade ne toe hh atret tons has tare tot ao at hhise tie ton seee sa nhos ha  he aos hir ned ho  oe sot ane aond sote\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_20_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f, map_location=lambda storage, loc: storage)\n",
    "\n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said to herself, to be answered, and to the club changes of his\n",
      "hat for the\n",
      "doctor's perfect woman when she could not be a pretty doctor's conversation. All the times on the churce of what it was the first time an old meant which, still seemed to her heart, but he could not be seen in speaking. She was chief that she went, and held his face all that he had nothing, and he felt that the money, she had been\n",
      "a stepped and a man who had been almost all\n",
      "the secretary, his eyes of the princess at the clear on the feeling that something were strange feeling for him, to ask an instant that he was this tone that he did\n",
      "not know them, but he saw at the table of his from her, as to her hands, he had not to\n",
      "decide. The\n",
      "strength of his shoted weight, to be told her face\n",
      "and he felt that it was a cried of\n",
      "his hands to\n",
      "his hands.\n",
      "\n",
      "\"You're seeing there about it.\"\n",
      "\n",
      "\"I have been at a\n",
      "little of taken,\" said Stepan Arkadyevitch, louking at her hands, and so struck\n",
      "his brother and talked with her hat, and as a carefully repont with his study with seciness with the steps of the statements at the memory,\n",
      "which had to be still to be the stop, he shaded him, the peasants, as it were, with a colored hearted\n",
      "looking approach and her\n",
      "sister's carriage was the station of socance. And so he said his frings and his stopped strange woman, said\n",
      "his\n",
      "brother.\n",
      "\n",
      "The convincions were so must be found\n",
      "herself, were a strange sound of the mistress\n",
      "and significantly only a cheeks, the cheer of the streams, she\n",
      "had taken up a sort, but were a partier, he was carrying himself out on the starts of all the country and his crowned attention.\n",
      "\n",
      "\"There's nothing it all\n",
      "at out that some seemed a men. And when I'm such a side of his\n",
      "fach,\n",
      "though I do the same, and what he can be decented, and I am to go into the stairs and with him. And you've cried them to be done?\" said Levin,\n",
      "liking his\n",
      "eyes off. The day to the sick coan had and the doctor was not the\n",
      "starl of his hands when she had been the forest and heally would ha\n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
