{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "Neural Network를 정의하고 훈련하여 MNIST 데이터 셋으로부터 손글씨 숫자를 인식하도록 하자.\n",
    "\n",
    "![img](../assets/function_approx.png)\n",
    "\n",
    "훈련 데이터로 신경망의 parameter (=weights,bias)를 우리가 원하는 답에 가깝게 근사하도록 해야 한다.\n",
    "\n",
    "이러한 parameters를 찾기 위해서,**loss function** (=cost)를 사용한다. 회귀문제에서 loss 함수는 mean squared loss를 사용하고 분류 문제를 위해서는 cross entropy를 주로 활용한다.\n",
    "\n",
    "신경망의 학습 목표는 이러한 loss를 최소화하는 parameter를 찾는 것에 있다. 이러한 과정은  **gradient descent**알고리즘을 활용한다. gradient 는 어떤 한점에서 loss function의 기울기를 말하고 이것은 해당 loss를 가장 빠르게 감소할수 있는 방향과 크기를 나타낸다. loss를 최소화하는 방향으로 진행하는 학습은 높은 산을 내려가는 방법과 비교할 수 있다.\n",
    "\n",
    "![img](../assets/gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "깊이가 깊은 multilayer neural network에서는 gradient를 계산하기 쉽지 않다. multilayer networks를 훈련하기 위한 다양한 연구가 진행되어 왔다.\n",
    "\n",
    "multilayer networks를 훈련하기 위한 방법이 **backpropagation** 이다. 궁극적으로 backpropagation은 합성함수 미분에서 chain rule를 구현하는 것이다. 이해하기 쉽도록 2 layer network의 computational graph로 표현하면 다음과 같다.\n",
    "![img](../assets/backprop_diagram.png)\n",
    "\n",
    "weights를 학습하기 위해 gradient descent를 사용한다. backpropagation은 loss를 backwards로 전파한다.\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
    "$$\n",
    "\n",
    "weights의 update 는 다음과 같이 수행된다. 이 때 $\\alpha$는 learning rate이다.  \n",
    "\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "learning rate $\\alpha$ 는 weight update steps으로 update 속도와 관련되어 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses in PyTorch\n",
    "\n",
    "`nn` module은 cross-entropy loss (`nn.CrossEntropyLoss`)와 같은 loss함수를 제공한다. 보통 `criterion`이라는 이름으로 할당한다. MNIST와 같은 분류 문제에서 출력층의 활성화 함수는 softmax function를 사용하여 각 class별 확률을 출력한다. loss함수로는 cross-entropy를 사용한다. \n",
    "\n",
    "이때 주의해야 할 점이 있다. [the documentation for `nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss),\n",
    "\n",
    "> `nn.LogSoftmax()` 와 `nn.NLLLoss()` 를 하나로 결합한 것이 cross-entropy이다.\n",
    ">\n",
    "> 신경망의 최종 출력은 각 class에 해당하는 score이다.\n",
    "\n",
    "즉, 신경망의 최종 출력은 softmax 활성화 함수 없이 그대로 score만을 출력하고, cross-entropy를 loss함수로 적용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoneko\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5, )),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('../data', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2935, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력층의 활성화 함수로 log-softmax `nn.LogSoftmax` 또는 `F.log_softmax` ([documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.LogSoftmax))를 적용하는 형태로 구성 가능하다. 그렇게 되면 출력되는 output에 exponential `torch.exp(output)`을 적용하여 class의 확률을 구할 수 있다. log-softmax output에 negative log likelihood loss, `nn.NLLLoss` ([documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss))를 loss함수로 적용한다.\n",
    "\n",
    ">**실습 :** log-softmax를 출력층의 활성화 함수로 지정하시오.  \n",
    "loss 함수는 negative log likelihood loss로 지정하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3048, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "## Solution\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our log-probabilities\n",
    "logps = model(images)\n",
    "# Calculate the loss with the logps and the labels\n",
    "loss = criterion(logps, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Torch 는 `autograd` module을 제공한다. `autograd`는 자동으로 텐서의 gradients 를 계산한다. `autograd`를 사용하여 parameters의 gradient를 구한다.  텐서를 생성할 때 `requires_grad = True` 를 쓴다. 또는  `x.requires_grad_(True)`를 사용한다.\n",
    "\n",
    "gradient의 자동 계산 기능을 비활성화 하기 위해서는 `torch.no_grad()` 를 사용한다.:\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "```\n",
    "\n",
    "`torch.set_grad_enabled(True|False)`를 사용하여 활성화/비활성화 할 수 있다.\n",
    "\n",
    "gradient를 계산하기 위해서는 backward() 메소드를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1469,  0.0058],\n",
      "        [ 0.8189, -0.3680]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3153e+00, 3.3706e-05],\n",
      "        [6.7053e-01, 1.3544e-01]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y`를 생성하는데 사용된 함수를 확인할 수 있다. `PowBackward0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x00000222E9E4C7F0>\n"
     ]
    }
   ],
   "source": [
    "## grad_fn shows the function that generated this variable\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd module 은 텐서에 수행된 operations을 추적하고 각각에 대한 gradient를 자동으로 계산할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5303, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient를 계산하기 전에는  `x` 와 `y` gradient는 비어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `.backward` method로 gradient를 계산한다. \n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5734,  0.0029],\n",
      "        [ 0.4094, -0.1840]])\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Autograd together\n",
    "\n",
    "PyTorch에서 신경망을 구현하면, 기본적으로 모든 parameters가 `requires_grad = True`로 초기화된다. 따라서 gradient를 계산하기 위해서는 `loss.backward()`만 호출해주면 된다. 이 gradient 값은 weights를 update하는데 사용된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images)\n",
    "loss = criterion(logps, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[-0.0021, -0.0021, -0.0021,  ..., -0.0021, -0.0021, -0.0021],\n",
      "        [ 0.0016,  0.0016,  0.0016,  ...,  0.0016,  0.0016,  0.0016],\n",
      "        [ 0.0001,  0.0001,  0.0001,  ...,  0.0001,  0.0001,  0.0001],\n",
      "        ...,\n",
      "        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002],\n",
      "        [-0.0011, -0.0011, -0.0011,  ..., -0.0011, -0.0011, -0.0011],\n",
      "        [-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005]])\n"
     ]
    }
   ],
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network!\n",
    "\n",
    "optimizer를 설정한다. optimizer는 앞서 구한 gradient값을 적용해서 parameter를 update한다. PyTorch의 [`optim` package](https://pytorch.org/docs/stable/optim.html)을 import한다. 예를 들어 stochastic gradient descent는 `optim.SGD`로 구현된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 data를 훈련하기에 앞서 loop를 1회만 수행해 보도록 하자.  \n",
    "\n",
    "일반적으로 PyTorch에서는 다음과 같은 절차로 수행한다.:\n",
    "\n",
    "* Make a forward pass through the network \n",
    "* Use the network output to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "각 단계에 대해 weight값을 출력해 본다. 이때 주의할 것은 `optimizer.zero_grad()` 부분이다. gradients는 호출될때 누적되어 계산된다. 따라서 각 loop에서 `optimizer.zero_grad()`를 호출하여 0으로 초기화 해 주어야만 정확한 gradient가 계산된다. 즉 각 batch별로 gradient를 loop 돌때 0으로 초기화 하라는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[ 0.0192,  0.0116, -0.0265,  ..., -0.0188, -0.0330, -0.0062],\n",
      "        [-0.0025, -0.0073,  0.0348,  ..., -0.0161,  0.0225,  0.0001],\n",
      "        [ 0.0181, -0.0261, -0.0060,  ...,  0.0097,  0.0200,  0.0125],\n",
      "        ...,\n",
      "        [ 0.0249, -0.0330, -0.0217,  ..., -0.0329, -0.0050, -0.0249],\n",
      "        [-0.0094,  0.0168,  0.0185,  ...,  0.0273, -0.0085,  0.0290],\n",
      "        [ 0.0323, -0.0199,  0.0018,  ..., -0.0064, -0.0182,  0.0032]],\n",
      "       requires_grad=True)\n",
      "Gradient - tensor([[-0.0002, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0002],\n",
      "        [ 0.0019,  0.0019,  0.0019,  ...,  0.0019,  0.0019,  0.0019],\n",
      "        [ 0.0012,  0.0012,  0.0012,  ...,  0.0012,  0.0012,  0.0012],\n",
      "        ...,\n",
      "        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002],\n",
      "        [-0.0027, -0.0027, -0.0027,  ..., -0.0027, -0.0027, -0.0027],\n",
      "        [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights -  Parameter containing:\n",
      "tensor([[ 1.9224e-02,  1.1638e-02, -2.6455e-02,  ..., -1.8764e-02,\n",
      "         -3.2991e-02, -6.1908e-03],\n",
      "        [-2.5492e-03, -7.3212e-03,  3.4785e-02,  ..., -1.6150e-02,\n",
      "          2.2458e-02,  8.1877e-05],\n",
      "        [ 1.8081e-02, -2.6156e-02, -6.0476e-03,  ...,  9.7314e-03,\n",
      "          1.9988e-02,  1.2515e-02],\n",
      "        ...,\n",
      "        [ 2.4887e-02, -3.2978e-02, -2.1697e-02,  ..., -3.2907e-02,\n",
      "         -5.0167e-03, -2.4932e-02],\n",
      "        [-9.3481e-03,  1.6870e-02,  1.8507e-02,  ...,  2.7354e-02,\n",
      "         -8.5007e-03,  2.9070e-02],\n",
      "        [ 3.2267e-02, -1.9864e-02,  1.8507e-03,  ..., -6.4268e-03,\n",
      "         -1.8193e-02,  3.2281e-03]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Take an update step and few the new weights\n",
    "optimizer.step()\n",
    "print('Updated weights - ', model[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for real\n",
    "\n",
    "실제로 test 데이터 셋을 가지고 전체 훈련과정을 구현해 보자. 이때 *epoch*은 전체 훈련 데이터를 처리하는 반복 횟수이다. `trainloader`에서 batches 단위로 데이터를 가져와서 각 batch별로 훈련을 진행한다.각 training pass별로 loss를 계산하고, backwards pass를 수행해서, weights를 update한다.\n",
    "\n",
    "> **실습 :** training pass를 구현하시오. 제대로 훈련이 진행된다면 각 epoch를 돌면서 loss가 줄어드는 것을 확인할 수 있다. (5개 epoch을 수행하는데 약 3분~5분정도 소요될 수 있다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 41.310361898275836\n",
      "Training loss: 13.432947668693721\n",
      "Training loss: 10.44403484987933\n",
      "Training loss: 8.41932541259062\n",
      "Training loss: 7.2527076909695865\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        # TODO: Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        \n",
    "        loss = criterion(output, labels)*images.size(0)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망의 훈련이 끝나면 제대로 분류하는지 테스트를 수행해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def view_classify(img, ps):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    \n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    \n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
